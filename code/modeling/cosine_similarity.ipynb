{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install sentence_transformers"
      ],
      "metadata": {
        "id": "3HGWdcFq0I9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HF-BVq1Uz8P2"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent1 = \"The location of the accommodation is a semi-basement accommodation in Korea that is easy to find and common.\"\n",
        "sent2 = \"The location of the accommodation is easy to find and it is a representative semi-basement accommodation in Korea.\"\n",
        "model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
        "\n",
        "def cosine(sent1, sent2):\n",
        "  sentences = [sent1,sent2]\n",
        "  #﻿(1) 임베딩된 두 문장을 구해줍니다.(여기서는 SBERT를 이용하여 임베딩 하였습니다.)\n",
        "  sentence_embeddings = model.encode(sentences)\n",
        "  #﻿(2) 공식을 적용시켜줍니다.\n",
        "  return np.dot(sentence_embeddings[0],sentence_embeddings[1])/(np.linalg.norm(sentence_embeddings[0])*np.linalg.norm(sentence_embeddings[1]))\n",
        "\n",
        "result = cosine(sent1,sent2)\n",
        "print(\"Cosine Similarity : \",result)"
      ],
      "metadata": {
        "id": "tj4H972nz8z7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "klue_sts_train = pd.read_csv(\"/content/SICK_train.csv\")"
      ],
      "metadata": {
        "id": "gH1Nb6-E0QOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "klue_sts_train = klue_sts_train.drop('Unnamed: 0', axis = 1)\n"
      ],
      "metadata": {
        "id": "WxIZ-w7j5bdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "klue_sts_train.head()"
      ],
      "metadata": {
        "id": "L30slxd_8o3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # A 문장과 B 문장이 모두 결합 되어 결과를 도출한다...\n",
        "\n",
        "# A = klue_sts_train['sentence_A']\n",
        "# B = klue_sts_train['sentence_B']\n",
        "# for i in A:\n",
        "#   for j in B:\n",
        "#     sent1 = i\n",
        "#     sent2 = j\n",
        "#     model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
        "\n",
        "#     def cosine(sent1, sent2):\n",
        "#       sentences = [sent1,sent2]\n",
        "#       #﻿(1) 임베딩된 두 문장을 구해줍니다.(여기서는 SBERT를 이용하여 임베딩 하였습니다.)\n",
        "#       sentence_embeddings = model.encode(sentences)\n",
        "#       #﻿(2) 공식을 적용시켜줍니다.\n",
        "#       return np.dot(sentence_embeddings[0],sentence_embeddings[1])/(np.linalg.norm(sentence_embeddings[0])*np.linalg.norm(sentence_embeddings[1]))\n",
        "\n",
        "#     result = cosine(sent1,sent2)\n",
        "#     print(sent1)\n",
        "#     print(sent2)\n",
        "#     print(\"Cosine Similarity : \",result)"
      ],
      "metadata": {
        "id": "uOELtcJ889SP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine(sent1, sent2):\n",
        "      sentences = [sent1,sent2]\n",
        "      #﻿(1) 임베딩된 두 문장을 구해줍니다.(여기서는 SBERT를 이용하여 임베딩 하였습니다.)\n",
        "      sentence_embeddings = model.encode(sentences)\n",
        "      #﻿(2) 공식을 적용시켜줍니다.\n",
        "      return np.dot(sentence_embeddings[0],sentence_embeddings[1])/(np.linalg.norm(sentence_embeddings[0])*np.linalg.norm(sentence_embeddings[1]))\n",
        "\n",
        "\n",
        "\n",
        "klue_sts_train['co_similarity'] = klue_sts_train.apply(lambda x: cosine(x['sentence_A'], x['sentence_B']), axis=1)"
      ],
      "metadata": {
        "id": "Z3h0hgUJAFRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "klue_sts_train.head(30)"
      ],
      "metadata": {
        "id": "3_MmFBGlCEeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "klue_sts_train.to_csv(\"klue_sts_train.csv\")"
      ],
      "metadata": {
        "id": "EtoIYUVUER96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y8CWeRjr-suu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}