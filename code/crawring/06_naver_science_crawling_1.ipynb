{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "06_naver_science_crawling_1.ipynb",
   "private_outputs": true,
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_KVLxWj9Uqj1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#import ssl"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "NOT_ACCEPT = 0\n",
    "\n",
    "def crawl_detail(href, category):\n",
    "    detail = requests.get(href)\n",
    "    detail_html = detail.content\n",
    "    detail_soup = BeautifulSoup(detail_html, 'html.parser')\n",
    "    detail_question = detail_soup.find('div', \"c-heading__content\")\n",
    "    \n",
    "    if detail_question is not None:\n",
    "        detail_question = detail_question.text.strip()\n",
    "    else:\n",
    "        detail_question = \"NaN\"\n",
    "    #content > div.question-content > div > div.c-heading._questionContentsArea.c-heading--default-old > div.c-heading__title > div > div\n",
    "\n",
    "    detail_answer = detail_soup.find('div', \"answer-content__list\")\n",
    "    \n",
    "    accepted_idx = NOT_ACCEPT\n",
    "    qa_list = [category, detail_question]\n",
    "    for x in range(1, 4):\n",
    "        answer = detail_answer.select_one(f'#answer_{x} > div.c-heading-answer__content > div.c-heading-answer__content-user')\n",
    "        if answer is not None:\n",
    "            qa_list.append(answer.text.strip())\n",
    "        else:\n",
    "            qa_list.append(\"NaN\")\n",
    "        \n",
    "        accept = detail_answer.select_one(f'#answer_{x} > div.c-heading-answer > div.c-heading-answer__body > div.adopt-check-box')\n",
    "        if accept is not None:\n",
    "            accepted_idx = x\n",
    "\n",
    "    qa_list.append(accepted_idx)\n",
    "    return qa_list"
   ],
   "metadata": {
    "id": "yETq3bH0UySd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# url -> 지식인 특정 카테고리에 Q&A를 긁는 기본 base url이다. 적어도 질문에 대한 답변이 하나는 존재한다\n",
    "# Q&A >교육,학문 > 중학교교육 > 사회  : 110302"
   ],
   "metadata": {
    "id": "OBci41goU2Pm",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def crawl_kin(page_n: int, category_num: int):\n",
    "    '''\n",
    "    기본적으로 게시판에는 20개의 게시글이 있고 총 긁어올 정보는 20 x page_n만큼의 QA 정보입니다.\n",
    "    category는 지식인의 어느 보드를 긁어올것인가에 대한 인자입니다.\n",
    "    '''\n",
    "    idx = 0\n",
    "    web_df = pd.DataFrame(columns=('category', 'question', 'answer_1', 'answer_2', 'answer_3', 'accept'))\n",
    "    detail_url = 'https://kin.naver.com/qna/detail.naver?'\n",
    "    \n",
    "    try:\n",
    "        for page in tqdm(range(1, page_n+1)):\n",
    "            url = f'https://kin.naver.com/qna/kinupList.naver?dirId={category_num}&page={page}'\n",
    "            \n",
    "            response = requests.get(url)\n",
    "            if response.status_code == 200:\n",
    "                html = response.text\n",
    "                soup = BeautifulSoup(html, 'html.parser')\n",
    "                ul = soup.find('tbody', id='au_board_list')\n",
    "                tr_list = ul.findAll('tr')\n",
    "\n",
    "                detail_list = []\n",
    "                for id, tr_tag in enumerate(tr_list, start=1):\n",
    "                    #한 게시판에 게시글이 20개씩있고 이것을 pagenation에 대해서 옮겨가야함\n",
    "                    title_tag = tr_tag.select('td.title > a')[0]\n",
    "                    category_tag = tr_tag.select('td.field > a')[0]\n",
    "                    title = title_tag.text\n",
    "                    detail_params = title_tag.get('href').split('?')[1]\n",
    "                    category = category_tag.text\n",
    "                    detail_href = detail_url + detail_params\n",
    "                    detail_list.append((detail_href, category))\n",
    "\n",
    "                for detail_href, category in detail_list:\n",
    "                    web_df_column = crawl_detail(detail_href, category)\n",
    "                    web_df.loc[idx] = web_df_column\n",
    "                    idx += 1\n",
    "\n",
    "            else:\n",
    "                print(response.status_code)\n",
    "                print(page)\n",
    "                break\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    web_df.to_csv(f\"./{page_n * 20}_{category_num}.csv\", index=False)\n",
    "\n",
    "\n",
    "crawl_kin(100, 110304)"
   ],
   "metadata": {
    "id": "aJdQI5N_U691",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "L9r8U3cSU-S0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}