{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_preprocessing_1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odNsJgppKTMv"
      },
      "outputs": [],
      "source": [
        "# preprocessing for SUBJ\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "# 해당 과정은 crawling.py 이후의 preprocessing 과정입니다.\n",
        "# 즉, 추출해야하는 명사, 형용사&동사의 단어를 네이버 사전으로부터 의미를 가져오고,\n",
        "# [명사], [형용사] & [동사] 에 포함되는 단어들로만 추려서 저장된 dataframe을 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_sub(df, noun_type):\n",
        "    \"\"\"\n",
        "    df : crawling DataFrame\n",
        "    noun_type: 명사의 어떤 noun type을  사용하는지 명시. 파일 저장 용도로 사용됨\n",
        "    \"\"\"\n",
        "\n",
        "    v2_dict = {'v2': []}\n",
        "    for desc in df['v1']:\n",
        "        # not noun이면 처리 안함\n",
        "        if desc == 'Not noun':\n",
        "            v2_dict['v2'].append('Not noun')\n",
        "            continue\n",
        "\n",
        "        # 여러 개의 뜻을 가지는 경우 존재\n",
        "        # 가장 우선순위 뜻만 사용할 것임으로 번호로 표기될 경우 \"1.\" 로 1번 뜻만 가져옴\n",
        "        if desc.startswith('1.'):\n",
        "            out = desc.split('1.')[1].split('.')[0]\n",
        "            # print('1 case: ', out)\n",
        "        elif desc.startswith('['):\n",
        "            out = desc.split(']')[1].split('.')[0]\n",
        "            # print('bracket case: ', out)\n",
        "        else:\n",
        "            out = desc.lstrip()\n",
        "\n",
        "        # 길이 길 경우 제거. 크롤링 할 때 길이 긴 설명은 페이지에 들어가야하기 때문\n",
        "        if len(out) > 57:\n",
        "            v2_dict['v2'].append('Longer')\n",
        "        else:\n",
        "            v2_dict['v2'].append(out)\n",
        "\n",
        "    # 유의어 처리\n",
        "    v3_dict = {'v3': []}\n",
        "    for desc in v2_dict['v2']:\n",
        "        # not noun이면 처리 안함\n",
        "        if desc in ['Not noun', 'Longer']:\n",
        "            v3_dict['v3'].append(desc)\n",
        "            continue\n",
        "\n",
        "        # 명사에 대해서는 유의어 처리를 일단 하지 않음 -> 이미 명사 너무 많음\n",
        "        if '[유의어]' in desc:\n",
        "            out = desc.split('[유의어]')[0]\n",
        "        else:\n",
        "            out = desc\n",
        "        v3_dict['v3'].append(out)\n",
        "\n",
        "    # 시작 단어에 특수문자 포함될 경우 제거\n",
        "    v4_dict = {'v4': []}\n",
        "    for desc in v3_dict['v3']:\n",
        "        # not noun이면 처리 안함\n",
        "        if desc in ['Not noun', 'Longer']:\n",
        "            v4_dict['v4'].append(desc)\n",
        "            continue\n",
        "        if '<' in desc or '→' in desc:\n",
        "            v4_dict['v4'].append('Start with special character')\n",
        "        else:\n",
        "            out = desc\n",
        "            v4_dict['v4'].append(out)\n",
        "\n",
        "    new_df = pd.concat([df, pd.DataFrame(v2_dict), pd.DataFrame(v3_dict), pd.DataFrame(v4_dict)], axis=1)\n",
        "    new_df.to_csv(f'./preprocessed_{noun_type}.csv')\n",
        "    print(f'{noun_type} done!')"
      ],
      "metadata": {
        "id": "Tk9mfAQmKfrk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_sim_words(word_lists):\n",
        "    split_words = word_lists.split(',')\n",
        "    new_words= []\n",
        "    for word in split_words:\n",
        "        if word[-1].isdigit():\n",
        "            new_words.append(word[:-1])\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return new_words\n"
      ],
      "metadata": {
        "id": "IAHM0fR5Kjqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_verb(df, verb_type):\n",
        "    \"\"\"\n",
        "    df : crawling DataFrame\n",
        "    noun_type: 명사의 어떤 noun type을  사용하는지 명시. 파일 저장 용도로 사용됨\n",
        "    \"\"\"\n",
        "\n",
        "    v2_dict = {'v2': []}\n",
        "    for desc in df['v1']:\n",
        "        # not noun이면 처리 안함\n",
        "        if desc == 'Not verb':\n",
        "            v2_dict['v2'].append(desc)\n",
        "            continue\n",
        "\n",
        "        # 1. 일 경우 1번 뜻만 가져온다\n",
        "        if desc.startswith('1.'):\n",
        "            out = desc.split('1.')[1].split('.')[0]\n",
        "            print('1 case: ', out)\n",
        "        elif desc.startswith('['):\n",
        "            out = desc.split(']')[1].split('.')[0]\n",
        "            print('bracket case: ', out)\n",
        "        else:\n",
        "            out = desc.lstrip()\n",
        "        # 길이 길 경우 제거. 크롤링 할 때 길이 긴 설명은 페이지에 들어가야하기 때문\n",
        "        if len(out) > 57:\n",
        "\n",
        "            v2_dict['v2'].append('Longer')\n",
        "        else:\n",
        "            v2_dict['v2'].append(out)\n",
        "\n",
        "    # 유의어 처리\n",
        "    v3_dict = {'v3': [], '유의어':[]}\n",
        "    for desc in v2_dict['v2']:\n",
        "        # not noun이면 처리 안함\n",
        "        if desc in ['Not verb', 'Longer']:\n",
        "            v3_dict['v3'].append(desc)\n",
        "            v3_dict['유의어'].append('NA')\n",
        "            continue\n",
        "        if '[유의어]' in desc:\n",
        "            out = desc.split('[유의어]')[0]\n",
        "            sim_words = desc.split('[유의어]')[1]\n",
        "            # 동사의 경우 유의어 column을 따로 만듭니다.\n",
        "            sim_words = process_sim_words(sim_words)\n",
        "        else:\n",
        "            out = desc\n",
        "            sim_words = 'NA'\n",
        "        v3_dict['v3'].append(out)\n",
        "        v3_dict['유의어'].append(sim_words)\n",
        "\n",
        "    # 시작 단어에 특수문자 포함될 경우 제거\n",
        "    v4_dict = {'v4': []}\n",
        "    for desc in v3_dict['v3']:\n",
        "        # not noun이면 처리 안함\n",
        "        if desc in ['Not verb', 'Longer']:\n",
        "            v4_dict['v4'].append(desc)\n",
        "            continue\n",
        "        if \"'\" in desc or '→' in desc:\n",
        "            v4_dict['v4'].append('Start with special character')\n",
        "        else:\n",
        "            out = desc\n",
        "            v4_dict['v4'].append(out)\n",
        "\n",
        "    new_df = pd.concat([df, pd.DataFrame(v2_dict), pd.DataFrame(v3_dict), pd.DataFrame(v4_dict)], axis=1)\n",
        "    new_df.to_csv(f'./preprocessed_{verb_type}.csv')\n",
        "    print(f'{verb_type} done!')\n"
      ],
      "metadata": {
        "id": "BzP2XZyaKmWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__=='__main__':\n",
        "    noun_type = ['NNP', 'NNG']\n",
        "    for noun in noun_type:\n",
        "        sub_df_path = f'/opt/ml/projects/notebooks/crawling_{noun}.csv'\n",
        "        sub_df = pd.read_csv(sub_df_path)\n",
        "        filter_sub(sub_df, noun)\n",
        "\n",
        "    verb_type = ['VV', 'VA']\n",
        "    for verb in verb_type:\n",
        "        verb_df_path = f'/opt/ml/projects/notebooks/crawling_{verb}.csv'\n",
        "        verb_df = pd.read_csv(verb_df_path)\n",
        "        filter_verb(verb_df, verb)\n"
      ],
      "metadata": {
        "id": "zdE80vmAKq1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CCwiR9CbKtTZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}