{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "03_gen_data_1.ipynb",
   "private_outputs": true,
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S5VF4-xmTcza",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## rule base generator\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# 동작 파악을 위해 효율성보다 verbose하게 작성하였습니다.\n",
    "class RuleBasedGenerator:\n",
    "    TEMP1 = ['sub', '조사', 'verb']        # sub : 주어\n",
    "    JOSA_LIST = ['은','는', '이', '가'] # 말이 안되더라도\n",
    "    NEG_LIST = ['안', '아니', '못']\n",
    "    def __init__(self, sub_df, verb_df, gen_pos_pair=2, gen_neg_pair=2, final_column_version='v4',antonym=True):\n",
    "        self.sub_df = sub_df\n",
    "        self.verb_df = verb_df\n",
    "        self.final_column_version = final_column_version\n",
    "        self.use_antonym = antonym\n",
    "        self.gen_pos_pair = gen_pos_pair\n",
    "        self.gen_neg_pair = gen_neg_pair\n",
    "\n",
    "        self.sub_list = self.sub_df['word'].tolist()\n",
    "        self.verb_list = self.verb_df['word'].tolist()\n",
    "        self._set_random()\n",
    "\n",
    "    def _set_random(self):\n",
    "        random.seed(42)\n",
    "        np.random.seed(42)\n",
    "\n",
    "    def get_random_sub(self, sub_list=None):\n",
    "        if sub_list is None:\n",
    "            sub_list = self.sub_list\n",
    "\n",
    "        while True:\n",
    "            sub_idx = random.choice(range(len(sub_list)))\n",
    "            if self.sub_df['v4'][sub_idx] != 'Not noun' and self.sub_df['v4'][sub_idx] != 'Longer' and \\\n",
    "                    self.sub_df['v4'][sub_idx] != 'Start with special character':\n",
    "                break\n",
    "            else:\n",
    "                print('selecting sub again!')\n",
    "\n",
    "        return sub_list[sub_idx], sub_idx\n",
    "\n",
    "    def get_random_verb(self, verb_list=None):\n",
    "        if verb_list is None:\n",
    "            verb_list = self.verb_list\n",
    "\n",
    "        while True:\n",
    "            verb_idx = random.choice(range(len(verb_list)))\n",
    "            if self.verb_df['v4'][verb_idx] != 'Not verb' and self.verb_df['v4'][verb_idx] != 'Longer' and \\\n",
    "                    self.verb_df['v4'][verb_idx] != 'Start with special character':\n",
    "                break\n",
    "            else:\n",
    "                print('selecting verb again!')\n",
    "        return verb_list[verb_idx], verb_idx\n",
    "\n",
    "    def gen_pos_data(self, sub_w, sub_idx, verb_w, verb_idx):\n",
    "        # '주어'의 단어를 뜻풀이로 변경\n",
    "        total_list = []\n",
    "        strs = ''\n",
    "        for temp in self.TEMP1:\n",
    "            if temp =='sub':\n",
    "                strs += self.sub_df[self.final_column_version][sub_idx]\n",
    "\n",
    "            elif temp =='조사':\n",
    "                josa = random.choice(self.JOSA_LIST)\n",
    "                strs += josa+' '\n",
    "\n",
    "            elif temp=='verb':\n",
    "                strs += verb_w\n",
    "        total_list.append(strs)\n",
    "\n",
    "        # '동사'의 단어를 뜻풀이로 변경\n",
    "        strs = ''\n",
    "        for temp in self.TEMP1:\n",
    "            if temp == 'sub':\n",
    "                strs += sub_w\n",
    "\n",
    "            elif temp == '조사':\n",
    "                josa = random.choice(self.JOSA_LIST)\n",
    "                strs += josa + ' '\n",
    "\n",
    "            elif temp == 'verb':\n",
    "                strs += self.verb_df[self.final_column_version][verb_idx]\n",
    "        total_list.append(strs)\n",
    "\n",
    "        # TODO '동사의 유의어를 선택\n",
    "\n",
    "        if not pd.isna(self.verb_df['유의어'][verb_idx]):\n",
    "            strs = ''\n",
    "            for temp in self.TEMP1:\n",
    "                if temp == 'sub':\n",
    "                    strs += sub_w\n",
    "\n",
    "                elif temp == '조사':\n",
    "                    josa = random.choice(self.JOSA_LIST)\n",
    "                    strs += josa + ' '\n",
    "\n",
    "                elif temp == 'verb':\n",
    "\n",
    "                    strs += eval(self.verb_df['유의어'][verb_idx])[0] # 유의어 들 중 첫번째 단어가 그래도 제일 유사하니까 첫번째 유의어만 일단 선택\n",
    "            total_list.append(strs)\n",
    "\n",
    "        # 만들어진 Pair 중 설정한 값만큼만 반환\n",
    "        return np.random.choice(total_list, self.gen_pos_pair,replace=False)\n",
    "\n",
    "    def gen_neg_data(self, sub_w, sub_idx, verb_w, verb_idx):\n",
    "        strs = ''\n",
    "        total_list = []\n",
    "        # 주어는 유지, 동사 앞에 부정부사 넣는 경우\n",
    "        # 안, 아니 -, 못 - , -'다'제거하고 -기지 않기 때문이다./ -지 않기 때문이다.\n",
    "        for temp in self.TEMP1:\n",
    "            if temp == 'sub':\n",
    "                strs += sub_w\n",
    "\n",
    "            elif temp == '조사':\n",
    "                josa = random.choice(self.JOSA_LIST)\n",
    "                strs += josa + ' '\n",
    "\n",
    "            elif temp == 'verb':\n",
    "                neg = random.choice(self.NEG_LIST)\n",
    "                strs += neg + ' ' + verb_w  # TODO :  -'다'제거하고 -기지 않기 때문이다./ -지 않기 때문이다.\n",
    "        total_list.append(strs)\n",
    "\n",
    "        # 주어를 뜻풀이로 바꾸고, 동사 앞에 부정부사 넣는 경우\n",
    "        strs = ''\n",
    "        for temp in self.TEMP1:\n",
    "            if temp == 'sub':\n",
    "                strs += self.sub_df[self.final_column_version][sub_idx]\n",
    "\n",
    "            elif temp == '조사':\n",
    "                josa = random.choice(self.JOSA_LIST)\n",
    "                strs += josa + ' '\n",
    "\n",
    "            elif temp == 'verb':\n",
    "                neg = random.choice(self.NEG_LIST)\n",
    "                strs += neg + ' ' + verb_w  # TODO :  -'다'제거하고 -기지 않기 때문이다./ -지 않기 때문이다.\n",
    "        total_list.append(strs)\n",
    "\n",
    "        # TODO 동사의 반의어 선택! 또는 다른 단어\n",
    "        #antonym = False\n",
    "        if self.use_antonym:\n",
    "            if not pd.isna(self.verb_df['반의어'][verb_idx]):\n",
    "                # breakpoint()\n",
    "                strs = ''\n",
    "                for temp in self.TEMP1:\n",
    "                    if temp == 'sub':\n",
    "                        strs += sub_w\n",
    "\n",
    "                    elif temp == '조사':\n",
    "                        josa = random.choice(self.JOSA_LIST)\n",
    "                        strs += josa + ' '\n",
    "\n",
    "                    elif temp == 'verb':\n",
    "                        strs += self.verb_df['반의어'][verb_idx] # 유의어 들 중 첫번째 단어가 그래도 제일 유사하니까 첫번째 유의어만 일단 선택\n",
    "                total_list.append(strs)\n",
    "\n",
    "        # 만들어진 Pair 중 설정한 값만큼만 반환\n",
    "        return np.random.choice(total_list, self.gen_neg_pair,replace=False)\n",
    "\n",
    "\n",
    "    def make_pairs(self):\n",
    "        # TODO : 단 한번도 뽑히지 않은 애들로만 뽑을 수 있게 하는 것\n",
    "\n",
    "        sub_w, sub_idx = self.get_random_sub()\n",
    "        verb_w, verb_idx = self.get_random_verb()\n",
    "        org_sents = ''\n",
    "        for temp in self.TEMP1:\n",
    "            if temp == 'sub':\n",
    "                org_sents += sub_w\n",
    "            elif temp == 'verb':\n",
    "                org_sents += verb_w\n",
    "            elif temp=='조사':\n",
    "                org_sents += '은' + ' ' # 기본형은 '은' 으로\n",
    "\n",
    "        pos_pairs = self.gen_pos_data(sub_w, sub_idx, verb_w, verb_idx)\n",
    "        neg_pairs = self.gen_neg_data(sub_w, sub_idx, verb_w, verb_idx)\n",
    "        return {'sub_w':sub_w, 'verb_w':verb_w, 'org_sents':org_sents, 'pos_pairs':pos_pairs, 'neg_pairs':neg_pairs}\n",
    "\n",
    "    def gen_data(self, num_itr=500, version_name='test'):\n",
    "        org_data_dict = {'sub_w':[],\n",
    "                           'verb_w':[],\n",
    "                           'org_sents':[],\n",
    "                           'pos_pairs':[],\n",
    "                           'neg_pairs':[]}\n",
    "\n",
    "        # 하지만 실제 데이터로 사용하려면..\n",
    "        csv_data = {'sent_a':[],\n",
    "                          'sent_b':[],\n",
    "                          'labels':[],\n",
    "                          'org_sents':[]} # org_sents는 식별 인자용\n",
    "        for itr in range(num_itr):\n",
    "            pair_out = self.make_pairs()\n",
    "            org_sents = pair_out['org_sents']\n",
    "            for k, v in org_data_dict.items():\n",
    "                org_data_dict[k].append(pair_out[k])\n",
    "\n",
    "            # 기본 조합  : org_sents + pos/ neg\n",
    "            for pos_idx in range(len(pair_out['pos_pairs'])):\n",
    "                csv_data['sent_a'].append(org_sents)\n",
    "                csv_data['sent_b'].append(pair_out['pos_pairs'][pos_idx])\n",
    "                csv_data['labels'].append(1)\n",
    "                csv_data['org_sents'].append(org_sents)\n",
    "\n",
    "            for neg_idx in range(len(pair_out['neg_pairs'])):\n",
    "                csv_data['sent_a'].append(org_sents)\n",
    "                csv_data['sent_b'].append(pair_out['neg_pairs'][neg_idx])\n",
    "                csv_data['labels'].append(0)\n",
    "                csv_data['org_sents'].append(org_sents)\n",
    "\n",
    "            # 좀더 생각한 조합 : pos<->pos, neg<->neg, pos<->neg\n",
    "            sent_a, sent_b = np.random.choice(pair_out['pos_pairs'], 2)\n",
    "            csv_data['sent_a'].append(sent_a)\n",
    "            csv_data['sent_b'].append(sent_b)\n",
    "            csv_data['labels'].append(1)\n",
    "            csv_data['org_sents'].append(org_sents)\n",
    "\n",
    "            sent_a, sent_b = np.random.choice(pair_out['neg_pairs'], 2)\n",
    "            csv_data['sent_a'].append(sent_a)\n",
    "            csv_data['sent_b'].append(sent_b)\n",
    "            csv_data['labels'].append(1)\n",
    "            csv_data['org_sents'].append(org_sents)\n",
    "\n",
    "            # 다른 pair에 대한 데이터가 더 만들어지게 될 것. -> 채점 입장에서 같은 것보다 얼마나 다르냐가 더 중요하니 그런 데이터를 더 모은다고 볼 수 있을까?\n",
    "            for pos_idx in range(len(pair_out['pos_pairs'])):\n",
    "                for neg_idx in range(len(pair_out['neg_pairs'])):\n",
    "                    csv_data['sent_a'].append(pair_out['pos_pairs'][pos_idx])\n",
    "                    csv_data['sent_b'].append(pair_out['neg_pairs'][neg_idx])\n",
    "                    csv_data['labels'].append(0)\n",
    "                    csv_data['org_sents'].append(org_sents)\n",
    "        # breakpoint()\n",
    "        df = pd.DataFrame(csv_data)\n",
    "        df.to_csv(f'./gen_data_{version_name}_{len(csv_data)}.csv')\n"
   ],
   "metadata": {
    "id": "r85u7Y8hTfIH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "if __name__=='__main__':\n",
    "    import pandas as pd\n",
    "    # sub_1 = pd.read_csv('/opt/ml/projects/final-project-level3-nlp-03/data_collection/preprocessed_NNG.csv').drop(columns=['Unnamed: 0'])\n",
    "    # sub_2 = pd.read_csv('/opt/ml/projects/final-project-level3-nlp-03/data_collection/preprocessed_NNP.csv').drop(columns=['Unnamed: 0'])\n",
    "    #\n",
    "    # verb_1 = pd.read_csv('/opt/ml/projects/final-project-level3-nlp-03/data_collection/preprocessed_VV.csv').drop(columns=['Unnamed: 0'])\n",
    "    # verb_2 = pd.read_csv('/opt/ml/projects/final-project-level3-nlp-03/data_collection/preprocessed_VA.csv').drop(columns=['Unnamed: 0'])\n",
    "    # sub_df = pd.concat([sub_1, sub_2],ignore_index=True)\n",
    "    # verb_df = pd.concat([verb_1, verb_2],ignore_index=True)\n",
    "\n",
    "  #  sub_1 = pd.read_csv('/opt/ml/projects/final-project-level3-nlp-03/data_collection/preprocessed_NNG.csv').drop(columns=['Unnamed: 0'])\n",
    "    sub_2 = pd.read_csv('/opt/ml/projects/final-project-level3-nlp-03/data_collection/preprocessed_NNG_v5.csv').drop(columns=['Unnamed: 0'])\n",
    "\n",
    "  #  verb_1 = pd.read_csv('/opt/ml/projects/final-project-level3-nlp-03/data_collection/preprocessed_VV.csv').drop(columns=['Unnamed: 0'])\n",
    "    verb_2 = pd.read_csv('/opt/ml/projects/final-project-level3-nlp-03/data_collection/preprocessed_VA_v5.csv').drop(columns=['Unnamed: 0'])\n",
    "  #  sub_df = pd.concat([sub_1, sub_2],ignore_index=True)\n",
    "  #  verb_df = pd.concat([verb_1, verb_2],ignore_index=True)\n",
    "\n",
    "    generator = RuleBasedGenerator(sub_2, verb_2,final_column_version='v5',antonym=True)\n",
    "    generator.gen_data(num_itr=1800)\n",
    "    print('Finished!')"
   ],
   "metadata": {
    "id": "-xMGp2WBTo9x",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}